{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 TAA\n",
    "\n",
    "## Expression Recognition with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#to load matlab mat files\n",
    "from scipy.io import loadmat\n",
    "# for neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelNames = [\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "labelNames = [\"happy\", \"sad\"]\n",
    "\n",
    "# Load Training data\n",
    "mat=loadmat(f\"../datasets/train/{'_'.join(labelNames)}.mat\")\n",
    "# mat is a dict with key \"X\" for x-values, and key \"y\" for y values\n",
    "X, y = mat[\"X\"], mat[\"y\"]\n",
    "\n",
    "# Load Dev Data\n",
    "matDev=loadmat(f\"../datasets/dev/{'_'.join(labelNames)}.mat\")\n",
    "X_valid, y_valid = matDev[\"X\"], matDev[\"y\"]\n",
    "\n",
    "# Load Test Data\n",
    "matTest=loadmat(f\"../datasets/test/{'_'.join(labelNames)}.mat\")\n",
    "X_test, y_test = matTest[\"X\"], matTest[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers\n",
    "m = X.shape[0] # number of training examples\n",
    "labels = np.max(y)+1 # number of labels\n",
    "features = X.shape[1:] # number of features per example\n",
    "\n",
    "print(f\"Loaded {m} traing examples with {labels} labels, each with {features} features (pixels).\")\n",
    "print(\"Labels are:\", ', '.join(labelNames))\n",
    "\n",
    "print(f\"Loaded {X_valid.shape[0]} dev examples.\")\n",
    "print(f\"Loaded {X_test.shape[0]} test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examplesNumber = 10\n",
    "fig, axis = plt.subplots(labels,examplesNumber,figsize=(18,2*labels))\n",
    "fig.subplots_adjust(top=1)\n",
    "\n",
    "# Foreach label, output 10 random images\n",
    "for i in range(labels):\n",
    "    for j in range(examplesNumber):\n",
    "        # Show image\n",
    "        axis[i,j].imshow(X[np.random.randint((m/labels)*(i),(m/labels)*(i+1)+1),:].reshape(48,48,order=\"F\").T, cmap=\"gray\") \n",
    "        # Hide axes\n",
    "        axis[i,j].set_xticks([])\n",
    "        axis[i,j].set_yticks([])\n",
    "        # Display title only on first\n",
    "        if j==0:\n",
    "            axis[i,j].set_title(labelNames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "Based on https://medium.com/nerd-for-tech/how-to-train-neural-networks-for-image-classification-part-1-21327fe1cc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important variables\n",
    "iterations = 200\n",
    "hiddenLayers = [20]\n",
    "hyperParameter = 0.1\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "modelSeq = []\n",
    "# Flattens each image (48x48) to 1x2304\n",
    "modelSeq.append(keras.layers.Flatten(input_shape = [features[0]]))\n",
    "# Hidden layers with relu activation function\n",
    "for h in hiddenLayers:\n",
    "    modelSeq.append(keras.layers.Dense(\n",
    "        h, \n",
    "        activation = \"relu\", \n",
    "        bias_regularizer= keras.regularizers.l2(hyperParameter) if hyperParameter else None\n",
    "    ))\n",
    "# Output layer with softmax activation function\n",
    "modelSeq.append(keras.layers.Dense(\n",
    "    labels, \n",
    "    activation = \"softmax\",\n",
    "    bias_regularizer= keras.regularizers.l2(hyperParameter) if hyperParameter else None\n",
    "))\n",
    "\n",
    "model = keras.models.Sequential(modelSeq)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "gradient = keras.optimizers.SGD(learning_rate=alpha)\n",
    "model.compile(\n",
    "    # Using sparse categorical crossentropy loss function\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    # Using stochastic gradient descent as gradient descent\n",
    "    optimizer = gradient,\n",
    "    # In addition to cost, we want accuracy to help understanding how the model is working \n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs = iterations,\n",
    "    batch_size = 128,\n",
    "    validation_data = (X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize = (16, 10))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.title(\"Neural network training metrics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the loss does not look like it has reached a minimum (being stable on the end), it suggests we can train the network further.\n",
    "\n",
    "> It is normal that the accuracy for the training set is higher than for the validation set, but they might be close. If not, there is overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(labels, pred, y):\n",
    "    \n",
    "    confusionTFPN = {}\n",
    "    confusionLabels = {}\n",
    "    \n",
    "    # Foreach emotion\n",
    "    for ie in range(len(labels)):\n",
    "        # TRUE AND FALSE POSITIVES AND NEGATIVES (TFPN)\n",
    "        \n",
    "        # Get indexes where emotion was predicted\n",
    "        ieIndexesPred = [i for i in range(pred.size) if pred[i]==ie]\n",
    "        ieIndexesNotPred = [i for i in range(pred.size) if pred[i]!=ie]\n",
    "\n",
    "        # Predicted, and Actual\n",
    "        TP = sum(pred[:,np.newaxis][ieIndexesPred]==y[ieIndexesPred])\n",
    "        # Predicted, but not actual\n",
    "        FP = sum(pred[:,np.newaxis][ieIndexesPred]!=y[ieIndexesPred])\n",
    "        # Not predicted, but actual\n",
    "        FN = sum(y[ieIndexesNotPred]==ie)\n",
    "        # Not predicted and not actual\n",
    "        TN = sum(y[ieIndexesNotPred]!=ie)\n",
    "\n",
    "        TP = TP[0] if TP else 0\n",
    "        FP = FP[0] if FP else 0\n",
    "        FN = FN[0] if FN else 0\n",
    "        TN = TN[0] if TN else 0\n",
    "        \n",
    "        confusionTFPN[labels[ie]] = {\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'FN': FN,\n",
    "            'TN': TN\n",
    "        }\n",
    "        \n",
    "        # CONFUSION WITH OTHER labels (Confusion)\n",
    "        # For emotion e\n",
    "        # Count the number of predictions made for all classes\n",
    "        # Foreach emotion, check how many times it has been predicted \n",
    "        \n",
    "        # Get indexes where emotion is real\n",
    "        ieIndexesY = [i for i in range(y.size) if y[i]==ie]\n",
    "        \n",
    "        # Foreach matching prediction, check what emotion was predicted\n",
    "        confusionLabels[labels[ie]] = {\n",
    "            e: sum(pred[:,np.newaxis][ieIndexesY]==labels.index(e))[0] for e in labels\n",
    "        }\n",
    "        \n",
    "    return confusionTFPN, confusionLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputConfusionTables(labels, testExamplesNumber, confusionTFPN, confusionLabels, axs, axsLine, axsTitle):\n",
    "     # TABLES\n",
    "    rows = tuple(labels)\n",
    "    \n",
    "    # TABLES / TFPN\n",
    "    # Output confusion matrix as plot table\n",
    "    data = [\n",
    "        [\n",
    "            f\"{vals['TP']} ({vals['TP']/testExamplesNumber*100:.2f}%)\",\n",
    "            f\"{vals['TN']} ({vals['TN']/testExamplesNumber*100:.2f}%)\",\n",
    "            f\"{vals['TP']+vals['TN']} ({(vals['TP']+vals['TN'])/testExamplesNumber*100:.2f}%)\",\n",
    "            f\"{vals['FP']} ({vals['FP']/testExamplesNumber*100:.2f}%)\",\n",
    "            f\"{vals['FN']} ({vals['FN']/testExamplesNumber*100:.2f}%)\",\n",
    "            f\"{vals['FP']+vals['FN']} ({(vals['FP']+vals['FN'])/testExamplesNumber*100:.2f}%)\",\n",
    "            vals['FP']+vals['FN']+vals['TP']+vals['TN']\n",
    "        ]\n",
    "        for _, vals in confusionTFPN.items()\n",
    "    ]\n",
    "    columns = ['TP (%)', 'TN (%)', 'T (%)', 'FP (%)', 'FN (%)', 'F (%)', f\"Total\"]\n",
    "    \n",
    "    # Colorize cells depending on value \n",
    "    cellColoursTF=plt.cm.Blues([\n",
    "        [ (vals['TP']+vals['TN'])/testExamplesNumber, (vals['FP']+vals['FN'])/testExamplesNumber ] for _, vals in confusionTFPN.items()\n",
    "    ])\n",
    "    cellColourPos=plt.cm.Greens([\n",
    "        [ vals['TP']/(vals['TP']+vals['FP']), vals['FP']/(vals['TP']+vals['FP']) ] for _, vals in confusionTFPN.items()\n",
    "    ])\n",
    "    cellColourNeg=plt.cm.Reds([\n",
    "        [ vals['TN']/(vals['TN']+vals['FN']), vals['FN']/(vals['TN']+vals['FN']) ] for _, vals in confusionTFPN.items()\n",
    "    ])\n",
    "    \n",
    "    cellColours = [\n",
    "        [\n",
    "            cellColourPos[i][0],\n",
    "            cellColourNeg[i][0],\n",
    "            cellColoursTF[i][0],\n",
    "            cellColourPos[i][1],\n",
    "            cellColourNeg[i][1],\n",
    "            cellColoursTF[i][1],\n",
    "            [0, 0, 0, 0]\n",
    "        ]\n",
    "        for i in range(len(rows))\n",
    "    ]\n",
    "    \n",
    "    the_table = axs[axsLine].table(\n",
    "      cellText=data,\n",
    "      rowLabels=rows,\n",
    "      colLabels=columns,\n",
    "      loc='center',\n",
    "      cellColours=cellColours\n",
    "    )\n",
    "    # the_table.scale(1.2, 1)\n",
    "    the_table.auto_set_font_size(False)\n",
    "    the_table.set_fontsize(20)\n",
    "    axs[axsLine].axis('off')\n",
    "    axs[axsLine].axis('tight')\n",
    "    axs[axsLine].set_title(f\"Confusion matrix {axsTitle}\", fontsize=20, pad=0)    \n",
    "    \n",
    "    # TABLES / Confusion    \n",
    "    # Output confusion matrix as plot table\n",
    "    data = [[o for _,o in others.items()] for _,others in confusionLabels.items()]\n",
    "    columns = rows\n",
    "    \n",
    "    # Colorize cells depending on value\n",
    "    vals = [o for _,others in confusionLabels.items() for _,o in others.items()]\n",
    "    normal = plt.Normalize(min(vals)-1, max(vals)+1)\n",
    "    cellColours=plt.cm.Blues(normal(data))\n",
    "        \n",
    "    the_table = axs[axsLine+1].table(\n",
    "      cellText=data,\n",
    "      rowLabels=rows,\n",
    "      colLabels=columns,\n",
    "      loc='center',\n",
    "      cellColours=cellColours\n",
    "    )\n",
    "    \n",
    "    the_table.set_fontsize(20)\n",
    "    # the_table.scale(1, 4)\n",
    "    axs[axsLine+1].axis('off')\n",
    "    axs[axsLine+1].axis('tight')\n",
    "    axs[axsLine+1].set_title(f\"True/Predicted {axsTitle}\", fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y predicted for test data set\n",
    "predict = model.predict(X_test)\n",
    "y_pred = np.array([np.argmax(p) for p in predict])\n",
    "\n",
    "# Compute confusions\n",
    "confusionTFPN, confusionLabels = confusionMatrix(labelNames, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as table\n",
    "fig, axs = plt.subplots(2,1)\n",
    "\n",
    "outputConfusionTables(labelNames, y_test.size, confusionTFPN, confusionLabels, axs, 0, \"WITHOUT regularization\")\n",
    "\n",
    "\n",
    "fig.set_size_inches(18,6*len(labelNames))\n",
    "fig.subplots_adjust(left=0.2, top=20)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our images classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "imagesPerLine = 4\n",
    "lines = math.ceil(len(y_test)/imagesPerLine)\n",
    "\n",
    "fig, axis = plt.subplots(lines,imagesPerLine,figsize=(18,2*lines))\n",
    "fig.subplots_adjust(top=1)\n",
    "\n",
    "right = 0\n",
    "for i in range(lines):\n",
    "    for j in range(imagesPerLine):\n",
    "        index = i*imagesPerLine + j\n",
    "        \n",
    "        if X_test.shape[0]<=index:\n",
    "            break\n",
    "        \n",
    "        # Show image\n",
    "        axis[i,j].imshow(X_test[index,:].reshape(48,48,order=\"F\").T, cmap=\"gray\") \n",
    "        \n",
    "        # Hide axes\n",
    "        axis[i,j].set_xticks([])\n",
    "        axis[i,j].set_yticks([])\n",
    "        # Display title with predicted (actual)\n",
    "        axis[i,j].set_title(f\"{labelNames[y_pred[index]]} ({labelNames[y_test[index][0]]})\")\n",
    "        \n",
    "        right += y_pred[index]==y_test[index][0]\n",
    "        \n",
    "fig.suptitle('Images prediction (actual) labels', fontsize=16, y=1.1) \n",
    "\n",
    "plt.savefig(f\"../datasets/images/home_made/predictions.png\")\n",
    "\n",
    "print(f\"Got {right} right predictions out of {len(y_pred)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
